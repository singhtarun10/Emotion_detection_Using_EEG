# Generating HTML documentation for the project

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Emotion Detection Using EEG - Autoencoder + CNN-BiLSTM Model</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            padding: 20px;
            background-color: #f4f4f4;
        }
        h1, h2, h3 {
            color: #333;
        }
        code {
            background-color: #f9f9f9;
            padding: 2px 5px;
            border-radius: 3px;
            color: #d6336c;
        }
        pre {
            background-color: #f9f9f9;
            padding: 10px;
            border-left: 5px solid #ccc;
        }
    </style>
</head>
<body>

<h1>Emotion Detection Using EEG: Autoencoder + CNN-BiLSTM Model</h1>

<h2>1. Introduction</h2>
<p>This research focuses on building an emotion detection system using EEG (Electroencephalography) signals. 
We utilize a hybrid deep learning model combining an autoencoder for dimensionality reduction and feature extraction, 
followed by a CNN-BiLSTM model for emotion classification. The autoencoder compresses high-dimensional EEG data, 
while the CNN-BiLSTM leverages both spatial (CNN) and temporal (LSTM) characteristics to detect emotions effectively.</p>

<h2>2. Data Preprocessing</h2>
<p>The input data consists of EEG brainwave signals with high dimensionality. To make the data manageable and 
extract meaningful features, we preprocess the data as follows:</p>
<ul>
    <li><b>Load the Dataset:</b> The EEG signals are loaded from CSV files where the features represent different channels of EEG data.</li>
    <li><b>Normalization:</b> The EEG features are normalized using a <code>StandardScaler</code> to ensure all data is scaled properly.</li>
    <li><b>Reshaping:</b> The 1D EEG signals are reshaped into 2D grids to prepare them for convolutional layers.</li>
</ul>

<h2>3. Autoencoder Model</h2>
<p>The autoencoder is used to reduce the high-dimensional EEG data into a lower-dimensional representation. The model is composed of an encoder and a decoder:</p>
<ul>
    <li><b>Encoder:</b> The encoder uses several convolutional layers to compress the data into a smaller feature representation.</li>
    <li><b>Decoder:</b> The decoder reconstructs the data from the encoded representation, ensuring the key features are preserved.</li>
</ul>
<pre><code>
def build_autoencoder(input_shape):
    input_layer = Input(shape=input_shape)
    
    # Encoder
    x = Conv2D(16, (3, 3), padding='same')(input_layer)
    x = ReLU()(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2), padding='same')(x)

    x = Conv2D(8, (3, 3), padding='same')(x)
    x = ReLU()(x)
    x = BatchNormalization()(x)
    encoded = MaxPooling2D((2, 2), padding='same')(x)

    # Decoder
    x = Conv2D(8, (3, 3), padding='same')(encoded)
    x = ReLU()(x)
    x = BatchNormalization()(x)
    x = UpSampling2D((2, 2))(x)

    x = Conv2D(16, (3, 3), padding='same')(x)
    x = ReLU()(x)
    x = BatchNormalization()(x)
    x = UpSampling2D((2, 2))(x)

    decoded = Conv2D(input_shape[2], (3, 3), activation='sigmoid', padding='same')(x)

    autoencoder = Model(input_layer, decoded)
    autoencoder.compile(optimizer=Adam(learning_rate=1e-3), loss='mse')

    return autoencoder
</code></pre>

<h2>4. CNN-BiLSTM Model</h2>
<p>The core emotion detection model uses a combination of Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) layers. 
This hybrid model captures both the spatial and temporal dynamics of EEG signals:</p>
<ul>
    <li><b>CNN Layers:</b> The CNN layers extract spatial features from the EEG signals.</li>
    <li><b>BiLSTM Layers:</b> The BiLSTM layers capture temporal dependencies and emotion patterns in EEG data over time.</li>
</ul>
<pre><code>
def build_cnn_bilstm(input_shape):
    input_layer = Input(shape=input_shape)
    
    # CNN Layers for Spatial Feature Extraction
    x = TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'))(input_layer)
    x = TimeDistributed(BatchNormalization())(x)
    x = TimeDistributed(MaxPooling2D((2, 2), padding='same'))(x)
    
    # BiLSTM Layers for Temporal Dependencies
    x = Bidirectional(LSTM(64, return_sequences=False))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.4)(x)
    
    # Fully Connected Layer
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.4)(x)
    output_layer = Dense(3, activation='softmax')(x)
    
    model = Model(input_layer, output_layer)
    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
</code></pre>

<h2>5. Training and Evaluation</h2>
<p>The training process for both the autoencoder and CNN-BiLSTM models involves the following steps:</p>
<ul>
    <li><b>Autoencoder Training:</b> The autoencoder is trained to reconstruct EEG data with minimized loss using the <code>Mean Squared Error (MSE)</code> loss function.</li>
    <li><b>Feature Extraction:</b> The trained autoencoder extracts key features from the EEG data.</li>
    <li><b>CNN-BiLSTM Training:</b> The extracted features are used to train the CNN-BiLSTM model for emotion classification. 
    Early stopping and learning rate scheduling are employed to optimize training and prevent overfitting.</li>
</ul>
<pre><code>
cnn_bilstm_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, 
                     class_weight=class_weights_dict, callbacks=[early_stopping, checkpoint])
</code></pre>

<h2>6. Results</h2>
<p>The CNN-BiLSTM model achieves excellent results in classifying emotions based on EEG signals, with a test accuracy of 98%. 
The model generalizes well to unseen data and shows balanced performance across all classes as confirmed by the confusion matrix and classification report.</p>

<h2>7. Conclusion</h2>
<p>The Autoencoder + CNN-BiLSTM model is highly effective for emotion detection using EEG signals. 
It successfully reduces the dimensionality of EEG data and captures both spatial and temporal features, making it ideal for this task.</p>






    <h1>Project Documentation for Research Paper: EEG-Based Emotion Classification Using CNN-BiLSTM</h1>

    <h2>Introduction</h2>
    <p>The objective of this project is to develop a deep learning model that classifies emotions using EEG (Electroencephalography) signals. EEG signals are widely used in brain-computer interfaces to decode brain states, and emotion detection is a key application for fields like mental health, human-computer interaction, and entertainment.</p>
    <p>Our approach combines feature extraction using an <strong>Autoencoder</strong> followed by a <strong>CNN-BiLSTM</strong> (Convolutional Neural Network + Bidirectional Long Short-Term Memory) model for emotion classification. The model successfully achieves a <strong>98% accuracy</strong> on the dataset provided.</p>

    <h2>Methodology</h2>

    <h3>1. Dataset:</h3>
    <p>The EEG dataset used for the project contains brainwave signals corresponding to three emotion classes: <em>positive, neutral</em>, and <em>negative</em>.</p>
    <p>Each data point represents a series of EEG signals captured across different sensors over time. Initially, the dataset was structured in CSV format, where features (EEG signals) were stored in columns, and the last column represented the emotion class labels.</p>

    <h3>2. Data Preprocessing:</h3>
    <p><strong>Feature Scaling</strong>: All features (EEG signals) were scaled using a <code>StandardScaler</code>. This step is crucial for CNNs and LSTMs, which are sensitive to the input data scale.</p>
    <pre><code>scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)</code></pre>

    <h3>3. Autoencoder for Feature Extraction:</h3>
    <p><strong>Purpose</strong>: The autoencoder serves as an unsupervised feature extraction tool, compressing the EEG signals while retaining the most critical features.</p>
    <p><strong>Architecture</strong>: 
        <ul>
            <li>The encoder consists of convolutional layers with a reduced number of filters (32, 16) to compress the data.</li>
            <li>The decoder reconstructs the original data from the compressed feature set.</li>
        </ul>
    </p>
    <p>The output of the encoder was later used as input to the CNN-BiLSTM model.</p>
    <pre><code>autoencoder.fit(features_reshaped, features_reshaped, epochs=50, batch_size=32)</code></pre>
    <p><strong>Result</strong>: The autoencoder successfully reduced the dimensionality of the input data, creating a feature set of shape <code>(2132, 26, 26, 16)</code>.</p>

    <h3>4. CNN-BiLSTM for Emotion Classification:</h3>
    <p><strong>CNN Layer</strong>:
        <ul>
            <li><strong>Purpose</strong>: Extract spatial features from EEG data (i.e., how different brain regions interact).</li>
            <li><strong>TimeDistributed Layers</strong>: CNN layers are wrapped in TimeDistributed layers to apply convolution across time steps (for EEG signals).</li>
        </ul>
    </p>
    <p><strong>BiLSTM Layer</strong>:
        <ul>
            <li><strong>Purpose</strong>: Capture temporal dependencies in EEG signals. The Bidirectional LSTM can capture both forward and backward relationships in the time-series data, making it more effective than a unidirectional LSTM.</li>
        </ul>
    </p>
    <p><strong>Fully Connected Layer</strong>:
        <ul>
            <li>Dense layers are added with ReLU activations and regularization (L2) to prevent overfitting.</li>
        </ul>
    </p>
    <p><strong>Softmax Output</strong>: The model ends with a softmax activation to classify EEG signals into one of the three emotional states.</p>
    <pre><code>model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])</code></pre>

    <h3>5. Training and Evaluation:</h3>
    <p><strong>Training</strong>: The CNN-BiLSTM model was trained for 50 epochs with a batch size of 32. A learning rate of 0.001 was used with the Adam optimizer. Early stopping was applied to prevent overfitting.</p>
    <p><strong>Validation</strong>: Validation accuracy during training consistently reached over <strong>97%</strong>, indicating strong generalization on unseen data.</p>

    <h3>6. Results:</h3>
    <p>The final model achieved <strong>98% accuracy</strong> on the test set. A detailed classification report showed strong performance across all three emotion classes (<em>positive</em>, <em>neutral</em>, <em>negative</em>), with high precision, recall, and F1-scores.</p>
    <p><strong>Confusion Matrix</strong>: The confusion matrix showed minimal misclassifications, confirming the robustness of the model.</p>

    <h2>Visualization</h2>
    <p>We performed two key visualizations during the project:</p>
    <ol>
        <li><strong>Data Visualization (Before and After Preprocessing)</strong>:
            <ul>
                <li><strong>Before Preprocessing</strong>: EEG signals in raw format had varying scales and ranges, making it unsuitable for neural networks.</li>
                <li><strong>After Preprocessing</strong>: The scaled and reshaped features were visualized, showing normalized and structured data suitable for the CNN-BiLSTM architecture.</li>
            </ul>
        </li>
        <li><strong>Training Performance Visualization</strong>:
            <ul>
                <li><strong>Training and Validation Accuracy</strong>: A plot showed consistent improvement in both training and validation accuracy, reaching 98% by epoch 12.</li>
                <li><strong>Loss Curves</strong>: Both training and validation loss decreased steadily, demonstrating good model convergence without overfitting.</li>
            </ul>
        </li>
    </ol>

    <h2>Conclusion</h2>
    <p>The model's 98% accuracy on EEG-based emotion detection showcases the power of combining feature extraction (Autoencoder) with deep learning architectures (CNN-BiLSTM). The model can effectively classify emotions into <em>positive</em>, <em>negative</em>, and <em>neutral</em> categories using raw EEG data.</p>

</body>
</html>